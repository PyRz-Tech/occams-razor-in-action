# occams-razor-in-action
I added x² to perfectly linear data…
and the "more complex" model literally got **worse** test R² than simple linear

(Yes, the polynomial fit the training noise perfectly. Yes, Occam’s Razor laughed in my face.)
Sometimes the dumbest model wins. 
This is the shortest proof I’ve ever written.

[![read](https://img.shields.io/badge/Read_Now-0A66C2?style=flat&logo=github&logoColor=white&label=&labelColor=0A66C2&color=0A66C2)](https://github.com/PyRz-Tech/regression-wtf/blob/main/negative_regression_coef_.md)
<br>
---
[![Back to Profile](https://img.shields.io/badge/Back_to_Profile-000000?style=flat&logo=github&logoColor=white)](https://github.com/PyRz-Tech)
[![Star if you learned ⭐](https://img.shields.io/badge/Star_if_you_learned_%E2%AD%90-000000?style=flat&logo=github&logoColor=white)](https://github.com/PyRz-Tech/regression-wtf)
